{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import diffusers\n",
    "from diffusers import DiffusionPipeline\n",
    "from diffusers import schedulers, AutoencoderKL, UNet2DConditionModel, PNDMScheduler, StableDiffusionPipeline\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import transformers.models\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "import utils\n",
    "import cv2, os, math\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import logging\n",
    "import itertools\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_name_or_path = \"stablediffusionapi/realistic-vision-v51\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_encoder: transformers.models.clip.modeling_clip.CLIPTextModel = CLIPTextModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"text_encoder\"\n",
    ")\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"vae\"\n",
    ")\n",
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "    pretrained_model_name_or_path, subfolder=\"unet\"\n",
    ")\n",
    "tokenizer: CLIPTokenizer = CLIPTokenizer.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    subfolder=\"tokenizer\",\n",
    ")\n",
    "# pipeline = DiffusionPipeline.from_pretrained(pretrained_model_name_or_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/45 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:17<00:00,  2.61it/s]\n"
     ]
    }
   ],
   "source": [
    "test_img = pipeline(\"Young girl sitting at a cafe, portrait, cute, detailed face\", width=264, height=384, num_inference_steps=45, negative_prompt=\"ugly face, deformed\").images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img.save('test3.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Textual Inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "placeholder_token = \"<xyz>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49408"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.add_tokens(placeholder_token)\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "placeholder_token_id = tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "placeholder_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextEmbeddings(\n",
       "  (token_embedding): Embedding(49409, 768)\n",
       "  (position_embedding): Embedding(77, 768)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_encoder.text_model.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edeecad964504797bea156377d4c56a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/.conda/envs/CS330_CUDA/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    pretrained_model_name_or_path,\n",
    "    text_encoder=text_encoder,\n",
    "    tokenizer=tokenizer,\n",
    "    vae=vae,\n",
    "    unet=unet,\n",
    ")\n",
    "pipeline.save_pretrained(\"chilloutmix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze everything but the MLP\n",
    "def freeze_params(params):\n",
    "    for param in params:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze vae and unet\n",
    "freeze_params(vae.parameters())\n",
    "freeze_params(unet.parameters())\n",
    "# Freeze all parameters except for the token embeddings in text encoder\n",
    "freeze_params(text_encoder.text_model.encoder.parameters())\n",
    "freeze_params(text_encoder.text_model.final_layer_norm.parameters())\n",
    "freeze_params(text_encoder.text_model.embeddings.position_embedding.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/202599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202599/202599 [00:00<00:00, 948485.90it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = utils.TextualInversionDataset('data', tokenizer, placeholder_token=placeholder_token)\n",
    "def create_dataloader(train_batch_size=1):\n",
    "    return DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/.conda/envs/CS330_CUDA/lib/python3.9/site-packages/diffusers/configuration_utils.py:239: FutureWarning: It is deprecated to pass a pretrained model name or path to `from_config`.If you were trying to load a scheduler, please use <class 'diffusers.schedulers.scheduling_pndm.PNDMScheduler'>.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.\n",
      "  deprecate(\"config-passed-as-path\", \"1.0.0\", deprecation_message, standard_warn=False)\n"
     ]
    }
   ],
   "source": [
    "embedding_predictor = utils.MetaTextInversion()\n",
    "noise_scheduler = PNDMScheduler.from_config(pretrained_model_name_or_path, subfolder=\"scheduler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"learning_rate\": 5e-04,\n",
    "    \"scale_lr\": True,\n",
    "    \"max_train_steps\": 750,\n",
    "    \"save_steps\": 250,\n",
    "    \"train_batch_size\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"mixed_precision\": \"fp16\",\n",
    "    \"seed\": 42,\n",
    "    \"output_dir\": \"meta-text-inversion-output\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger()\n",
    "\n",
    "# def save_progress(text_encoder, placeholder_token_id, accelerator, save_path):\n",
    "#     logger.info(\"Saving embeddings\")\n",
    "#     learned_embeds = accelerator.unwrap_model(text_encoder).get_input_embeddings().weight[placeholder_token_id]\n",
    "#     learned_embeds_dict = {placeholder_token: learned_embeds.detach().cpu()}\n",
    "#     torch.save(learned_embeds_dict, save_path)\n",
    "\n",
    "def training_function(text_encoder, vae, unet, global_step=0):\n",
    "    train_batch_size = hyperparameters[\"train_batch_size\"]\n",
    "    gradient_accumulation_steps = hyperparameters[\"gradient_accumulation_steps\"]\n",
    "    learning_rate = hyperparameters[\"learning_rate\"]\n",
    "    max_train_steps = hyperparameters[\"max_train_steps\"]\n",
    "    output_dir = hyperparameters[\"output_dir\"]\n",
    "    gradient_checkpointing = hyperparameters[\"gradient_checkpointing\"]\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "        mixed_precision=hyperparameters[\"mixed_precision\"]\n",
    "    )\n",
    "\n",
    "    if gradient_checkpointing:\n",
    "        text_encoder.gradient_checkpointing_enable()\n",
    "        unet.enable_gradient_checkpointing()\n",
    "\n",
    "    train_dataloader = create_dataloader(train_batch_size)\n",
    "\n",
    "    if hyperparameters[\"scale_lr\"]:\n",
    "        learning_rate = (\n",
    "            learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        itertools.chain(\n",
    "            embedding_predictor.parameters(),\n",
    "            text_encoder.get_input_embeddings().parameters()\n",
    "        ),  # only optimize the embeddings and the embedding predictor MLP\n",
    "        lr=learning_rate,\n",
    "    )\n",
    "\n",
    "    text_encoder, optimizer, train_dataloader = accelerator.prepare(\n",
    "        text_encoder, optimizer, train_dataloader\n",
    "    )\n",
    "\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Move vae and unet to device\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    embedding_predictor.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    # Keep vae in eval mode as we don't train it\n",
    "    vae.eval()\n",
    "    # Keep unet in train mode to enable gradient checkpointing\n",
    "    unet.train()\n",
    "\n",
    "    # Train embedding predictor\n",
    "    embedding_predictor.train()\n",
    "\n",
    "    \n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "    num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {max_train_steps}\")\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm_notebook(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    for epoch in range(num_train_epochs):\n",
    "        text_encoder.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if torch.min(batch['valid']) == 0: continue\n",
    "            with accelerator.accumulate(text_encoder):\n",
    "                # Convert images to latent space\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample().detach()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device).long()\n",
    "\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                # Get the text embedding for conditioning\n",
    "                embeddings = text_encoder.get_input_embeddings().weight.data\n",
    "                mapped_embeddings = embedding_predictor(batch[\"face_id\"].to(dtype=weight_dtype))\n",
    "                batched_hidden_states = []\n",
    "                for b in range(train_batch_size):\n",
    "                    embeddings[placeholder_token_id] = mapped_embeddings[b]\n",
    "                    encoder_hidden_states = text_encoder(batch[\"input_ids\"][[b]])[0]\n",
    "                    batched_hidden_states.append(encoder_hidden_states)\n",
    "                batched_hidden_states = torch.vstack(batched_hidden_states)\n",
    "\n",
    "                # Predict the noise residual\n",
    "                noise_pred = unet(noisy_latents, timesteps, batched_hidden_states.to(weight_dtype)).sample\n",
    "\n",
    "                 # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                loss = F.mse_loss(noise_pred, target, reduction=\"none\").mean([1, 2, 3]).mean()\n",
    "                accelerator.backward(loss)\n",
    "\n",
    "                # Zero out the gradients for all token embeddings except the newly added\n",
    "                # embeddings for the concept, as we only want to optimize the concept embeddings\n",
    "                if accelerator.num_processes > 1:\n",
    "                    grads = text_encoder.module.get_input_embeddings().weight.grad\n",
    "                else:\n",
    "                    grads = text_encoder.get_input_embeddings().weight.grad\n",
    "                # Get the index for tokens that we want to zero the grads for\n",
    "                index_grads_to_zero = torch.arange(len(tokenizer)) != placeholder_token_id\n",
    "                grads.data[index_grads_to_zero, :] = grads.data[index_grads_to_zero, :].fill_(0)\n",
    "\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step % hyperparameters[\"save_steps\"] == 0:\n",
    "                    save_path = os.path.join(output_dir, f\"learned_embeds-step-{global_step}.bin\")\n",
    "                    # save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n",
    "                    torch.save(embedding_predictor.state_dict(), os.path.join(output_dir, f'mlp-{global_step}.bin'))\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item()}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "\n",
    "            if global_step >= max_train_steps:\n",
    "                break\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "\n",
    "\n",
    "    # Create the pipeline using using the trained modules and save it.\n",
    "    if accelerator.is_main_process:\n",
    "        pipeline = StableDiffusionPipeline.from_pretrained(\n",
    "            pretrained_model_name_or_path,\n",
    "            text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "            tokenizer=tokenizer,\n",
    "            vae=vae,\n",
    "            unet=unet,\n",
    "        )\n",
    "        pipeline.save_pretrained(output_dir)\n",
    "        # Also save the newly trained embeddings\n",
    "        # save_path = os.path.join(output_dir, f\"learned_embeds.bin\")\n",
    "        torch.save(embedding_predictor.state_dict(), os.path.join(output_dir, f'mlp-final.bin'))\n",
    "        # save_progress(text_encoder, placeholder_token_id, accelerator, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Launching training on one GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4064/1439260201.py:80: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  progress_bar = tqdm_notebook(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a20327ea6125442086095612c060c699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "safety_checker/model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b56171ce2dfd4fcbbbf0511368ecffce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import accelerate\n",
    "accelerate.notebook_launcher(training_function, args=(text_encoder, vae, unet, 500), num_processes=1)\n",
    "\n",
    "for param in itertools.chain(unet.parameters(), text_encoder.parameters()):\n",
    "  if param.grad is not None:\n",
    "    del param.grad  # free some memory\n",
    "  torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e690c737ab54560808e4479faeadd12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\n",
    "    \"attempt-ghostnet\",\n",
    "    scheduler=PNDMScheduler.from_pretrained(\"attempt-ghostnet\", subfolder=\"scheduler\"),\n",
    "    torch_dtype=torch.float16,\n",
    ").to(\"cuda\")\n",
    "pipe.safety_checker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7224d43649459a8fc5b345fa169460",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/.conda/envs/CS330_CUDA/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# chillout mix\n",
    "pipe: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(\"chilloutmix\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "pipe.safety_checker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/202599 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 202599/202599 [00:00<00:00, 1719017.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['valid', 'face_id', 'input_ids', 'prompt', 'pixel_values'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = utils.TextualInversionDataset('data', pipe.tokenizer, placeholder_token=placeholder_token, skip_faceid=False)\n",
    "dataset[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_face_id = dataset[20][\"face_id\"] # utils.get_face_embedding(\"original-test/test2.png\")[-1]\n",
    "# target_face_id = utils.get_face_embedding(\"out-6300/1-f1.png\")[-1]\n",
    "embedding_predictor = utils.MetaTextInversion()\n",
    "embedding_predictor.eval().to('cuda', dtype=torch.float16)\n",
    "embedding_predictor.load_state_dict(torch.load(\"attempt4/mlp-1900.bin\"))\n",
    "with torch.no_grad():\n",
    "    target_embedding = embedding_predictor(torch.tensor(target_face_id).to('cuda', dtype=torch.float16).unsqueeze(0))\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 178, 178])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1986/2411984371.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  example = torch.tensor(dataset[0]['pixel_values']).to('cuda', dtype=torch.float16).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "from ghostfacenetsv2 import GhostFaceNetsV2\n",
    "embedding_predictor = GhostFaceNetsV2(image_size=178, num_classes=768, dropout=0).eval().to('cuda', dtype=torch.float16)\n",
    "embedding_predictor.load_state_dict(torch.load(\"attempt-ghostnet/ghostnet-6300.bin\"))\n",
    "with torch.no_grad():\n",
    "    example = torch.tensor(dataset[0]['pixel_values']).to('cuda', dtype=torch.float16).unsqueeze(0)\n",
    "    print(example.shape)\n",
    "    target_embedding = embedding_predictor(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embedding_predictor\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49408"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "placeholder_token_id = pipe.tokenizer.convert_tokens_to_ids(placeholder_token)\n",
    "placeholder_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.tokenizer.convert_tokens_to_ids(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bab303c36e4c3e84495c828926a95a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in [5]:\n",
    "    target_face_id = dataset[i][\"face_id\"] # utils.get_face_embedding(\"original-test/test2.png\")[-1]\n",
    "    # target_face_id = utils.get_face_embedding(f\"poolf/1700-9-f7.png\")[-1]\n",
    "    embedding_predictor = utils.MetaTextInversion()\n",
    "    embedding_predictor.eval().to('cuda', dtype=torch.float16)\n",
    "    embedding_predictor.load_state_dict(torch.load(\"attempt4/mlp-1900.bin\"))\n",
    "    with torch.no_grad():\n",
    "        target_embedding = embedding_predictor(torch.tensor(target_face_id).to('cuda', dtype=torch.float16).unsqueeze(0))\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    embeddings = pipe.text_encoder.get_input_embeddings().weight.data\n",
    "    embeddings[placeholder_token_id] = target_embedding[0] # exclude batch dimension\n",
    "    for j in [1]:\n",
    "        conditioned_img = pipe(f\"a photo of {placeholder_token}, cute girl, beautiful, film grain, natural lighting, 8K UHD\", width=360, height=480, num_inference_steps=50, negative_prompt=\"disfigured, ugly, bad, cartoon, anime, 3d, painting, blurry\", safety_checker=None).images[0]\n",
    "        conditioned_img.save(f'test/conditioned/${j}_.png')\n",
    "        # conditioned_img.save(f'poolf/1900-{i}-{j}.png')\n",
    "        # conditioned_img = pipe(f\"{placeholder_token}, a photo of a {placeholder_token}, film grain, natural lighting, sharp focus, 8K UHD, handsome\", width=384, height=512, num_inference_steps=50, negative_prompt=\"disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, blurry, woman\", safety_checker=None).images[0]\n",
    "        # conditioned_img.save(f'poolm-chilloutmix/1900-{i}-f{j}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a35c8afca4a483d988c9c9a2671f41f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Replace embedding with target\n",
    "for i in [4]:\n",
    "    # embeddings = pipe.text_encoder.get_input_embeddings().weight.data\n",
    "    # embeddings[placeholder_token_id] = target_embedding[0] # exclude batch dimension\n",
    "    # 2-4\n",
    "    # conditioned_img = pipe(f\"one man, film grain, sharp focus, 8K UHD, handsome\", width=384, height=512, num_inference_steps=50, negative_prompt=\"disfigured, ugly, bad, immature, cartoon, anime, 3d, painting, blurry\", safety_checker=None).images[0]\n",
    "    # conditioned_img = pipe(f\"a girl, cute, film grain, natural lighting, 8K UHD\", width=360, height=480, num_inference_steps=50, negative_prompt=\"disfigured, ugly, bad, cartoon, anime, 3d, painting\", safety_checker=None).images[0]\n",
    "    # conditioned_img = pipe(f\"a girl, cute, beautiful, rim lighting, natural lighting, dslr, ultra quality, sharp focus, tack sharp, film grain, Fujifilm XT3, crystal clear, 8K UHD, detailed glossy eyes, high detailed skin, skin pores, teen, [Kpop idol]\", width=504, height=640, num_inference_steps=46,\n",
    "    #                        negative_prompt=\"disfigured, ugly, bad, cartoon, anime, 3d, painting, watermark, extra limbs, monochrome, grayscale, skin spots, pubic hair, unclear eyes\", safety_checker=None).images[0]\n",
    "    # conditioned_img = pipe(f\"a girl, cute, beautiful, rim lighting, natural lighting, dslr, ultra quality, sharp focus, tack sharp, film grain, crystal clear, 8K UHD, detailed glossy eyes, high detailed skin, skin pores, teen, (Kpop idol:0.8)\", width=592, height=792, num_inference_steps=55,\n",
    "    #                        negative_prompt=\"disfigured, ugly, bad, cartoon, anime, 3d, painting, watermark, monochrome, blurry, extra limbs, pubic hair, unclear eyes\", safety_checker=None).images[0]\n",
    "    conditioned_img.save(f'test/vanilla/{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioned_img.save('out-1900-1-4.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "unet/diffusion_pytorch_model.safetensors not found\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c275205f5ab94d749c15f2deba91fed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jerry/.conda/envs/CS330_CUDA/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline: StableDiffusionPipeline = StableDiffusionPipeline.from_pretrained(pretrained_model_name_or_path, torch_dtype=torch.float16).to('cuda')\n",
    "pipeline.safety_checker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_similarity(dir: str):\n",
    "    files = sorted(os.listdir(dir))\n",
    "    embeds = [utils.get_face_embedding(os.path.join(dir, files[i]))[1] for i in range(len(files)) if not files[i].endswith(\"1.png\")]\n",
    "    if np.min(np.max(embeds, axis=1)) == 0:\n",
    "        print(\"detected 0 somewhere\")\n",
    "        print(np.array(embeds)[:, :2])\n",
    "        return\n",
    "    sims = []\n",
    "    for i in tqdm_notebook(range(len(embeds) - 1)):\n",
    "        for j in range(i, len(embeds)):\n",
    "            sims.append(utils.cosine_similarity(embeds[i], embeds[j]))\n",
    "    \n",
    "    return np.mean(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21808/542144116.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(embeds) - 1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbe2c3735b5b4d39be9cc4be4fbba57e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9429690063392598"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_similarity(\"test/vanilla\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21808/2603963463.py:9: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i in tqdm_notebook(range(len(embeds) - 1)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89bf6acc917c4d23bc43003cc5917cb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9539687089097488"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_similarity(\"test/conditioned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS330_CUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
